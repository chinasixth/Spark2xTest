package com.qinglianyun.spark.sparkml.featuretransformers

import org.apache.spark.ml.feature.{Interaction, VectorAssembler}
import org.apache.spark.sql.{DataFrame, SparkSession}

/**
  * @ Author ：liuhao
  * @ Company: qinglianyun
  * @ Date   ：Created in 11:02 2019/1/15
  * @ desc: Interaction是一个转换器，它接收双值列或向量，生成一个向量列，其中包含一个向量列和所有组合的乘积
  * 假设有两列向量，具体计算方法：
  * 两个向量列的相同行进行乘积，用第一个向量的第一个列值依次乘以第二个向量所有的列值，还要乘以行号。
  */
object InteractionTest {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName(this.getClass.getName)
      .master("local[*]")
      .getOrCreate()

    val dataset: DataFrame = spark.createDataFrame(
      Seq(
        (1, 1, 2, 3, 8, 4, 5),
        (2, 4, 3, 8, 7, 9, 8),
        (3, 6, 1, 9, 2, 3, 6),
        (4, 10, 8, 6, 9, 4, 5),
        (5, 9, 2, 7, 10, 7, 3),
        (6, 1, 1, 4, 2, 8, 4)
      )
    ).toDF("id1", "id2", "id3", "id4", "id5", "id6", "id7")
    // +---+---+---+---+---+---+---+
    // |id1|id2|id3|id4|id5|id6|id7|
    // +---+---+---+---+---+---+---+
    // |1  |1  |2  |3  |8  |4  |5  |
    // |2  |4  |3  |8  |7  |9  |8  |
    // |3  |6  |1  |9  |2  |3  |6  |
    // |4  |10 |8  |6  |9  |4  |5  |
    // |5  |9  |2  |7  |10 |7  |3  |
    // |6  |1  |1  |4  |2  |8  |4  |
    // +---+---+---+---+---+---+---+

    val assembler1: VectorAssembler = new VectorAssembler()
      .setInputCols(Array("id2", "id3", "id4"))
      .setOutputCol("vec1")

    val df1: DataFrame = assembler1.transform(dataset)
    // +---+---+---+---+---+---+---+--------------+
    // |id1|id2|id3|id4|id5|id6|id7|vec1          |
    // +---+---+---+---+---+---+---+--------------+
    // |1  |1  |2  |3  |8  |4  |5  |[1.0,2.0,3.0] |
    // |2  |4  |3  |8  |7  |9  |8  |[4.0,3.0,8.0] |
    // |3  |6  |1  |9  |2  |3  |6  |[6.0,1.0,9.0] |
    // |4  |10 |8  |6  |9  |4  |5  |[10.0,8.0,6.0]|
    // |5  |9  |2  |7  |10 |7  |3  |[9.0,2.0,7.0] |
    // |6  |1  |1  |4  |2  |8  |4  |[1.0,1.0,4.0] |
    // +---+---+---+---+---+---+---+--------------+

    val assembler2: VectorAssembler = new VectorAssembler()
      .setInputCols(Array("id5", "id6", "id7"))
      .setOutputCol("vec2")

    val df2: DataFrame = assembler2.transform(df1)
    // +---+---+---+---+---+---+---+--------------+--------------+
    // |id1|id2|id3|id4|id5|id6|id7|vec1          |vec2          |
    // +---+---+---+---+---+---+---+--------------+--------------+
    // |1  |1  |2  |3  |8  |4  |5  |[1.0,2.0,3.0] |[8.0,4.0,5.0] |
    // |2  |4  |3  |8  |7  |9  |8  |[4.0,3.0,8.0] |[7.0,9.0,8.0] |
    // |3  |6  |1  |9  |2  |3  |6  |[6.0,1.0,9.0] |[2.0,3.0,6.0] |
    // |4  |10 |8  |6  |9  |4  |5  |[10.0,8.0,6.0]|[9.0,4.0,5.0] |
    // |5  |9  |2  |7  |10 |7  |3  |[9.0,2.0,7.0] |[10.0,7.0,3.0]|
    // |6  |1  |1  |4  |2  |8  |4  |[1.0,1.0,4.0] |[2.0,8.0,4.0] |
    // +---+---+---+---+---+---+---+--------------+--------------+

    val interaction: Interaction = new Interaction()
      .setInputCols(Array("vec1", "vec2"))
      .setOutputCol("interactionCol")

    val df3: DataFrame = interaction.transform(df2).select("id1", "vec1", "vec2", "interactionCol")

    df3.show(false)
    // 用vec1第一行的1，乘以8乘以1（行号），然后依次乘以4、5
    // +---+--------------+--------------+----------------------------------------------+
    // |id1|vec1          |vec2          |interactionCol                                |
    // +---+--------------+--------------+----------------------------------------------+
    // |1  |[1.0,2.0,3.0] |[8.0,4.0,5.0] |[8.0,4.0,5.0,16.0,8.0,10.0,24.0,12.0,15.0]    |
    // |2  |[4.0,3.0,8.0] |[7.0,9.0,8.0] |[28.0,36.0,32.0,21.0,27.0,24.0,56.0,72.0,64.0]|
    // |3  |[6.0,1.0,9.0] |[2.0,3.0,6.0] |[12.0,18.0,36.0,2.0,3.0,6.0,18.0,27.0,54.0]   |
    // |4  |[10.0,8.0,6.0]|[9.0,4.0,5.0] |[90.0,40.0,50.0,72.0,32.0,40.0,54.0,24.0,30.0]|
    // |5  |[9.0,2.0,7.0] |[10.0,7.0,3.0]|[90.0,63.0,27.0,20.0,14.0,6.0,70.0,49.0,21.0] |
    // |6  |[1.0,1.0,4.0] |[2.0,8.0,4.0] |[2.0,8.0,4.0,2.0,8.0,4.0,8.0,32.0,16.0]       |
    // +---+--------------+--------------+----------------------------------------------+

    spark.close()
  }

}
